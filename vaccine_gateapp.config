[BERT]
trainable_layers = encoder.layer.23.attention.self.query.weight,encoder.layer.23.attention.self.query.bias,encoder.layer.23.attention.self.key.weight,encoder.layer.23.attention.self.key.bias,encoder.layer.23.attention.self.value.weight,encoder.layer.23.attention.self.value.bias,encoder.layer.23.attention.output.dense.weight,encoder.layer.23.attention.output.dense.bias,encoder.layer.23.intermediate.dense.weight,encoder.layer.23.intermediate.dense.bias,encoder.layer.23.output.dense.weight,encoder.layer.23.output.dense.bias
bert_path = digitalepidemiologylab/covid-twitter-bert-v2
bert_dim = 1024
num_layers = 24

[MODEL]
hidden_dim = 300
z_dim = 50
ntopics = 50
class_topic_loss_lambda = 1
classification_loss_lambda = 1
banlance_loss = yes
gpu = no
[TARGET]
labels = LF,DPA,SEN,PE,Cons,MRE
